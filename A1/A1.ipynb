{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scientists often need to crawl data from websites and turn the crawled data (HTML pages) to structured data (tables). Thus, web scraping is an essential skill that every data scientist should master. In this assignment, you will learn the followings:\n",
    "\n",
    "\n",
    "* How to download HTML pages from a website?\n",
    "* How to extract relevant content from an HTML page? \n",
    "\n",
    "Furthermore, you will gain a deeper understanding of the data science lifecycle.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Please use [pandas.DataFrame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) rather than spark.DataFrame to manipulate data.\n",
    "\n",
    "2. Please use [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) rather than [lxml](http://lxml.de/) to parse an HTML page and extract data from the page.\n",
    "\n",
    "3. Please follow the python code style (https://www.python.org/dev/peps/pep-0008/). If TA finds your code hard to read, you will lose points. This requirement will stay for the whole semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is your first time to write a web scraper, you need to learn some basic knowledge of this topic. I found that this is a good resource: [Tutorial: Web Scraping and BeautifulSoup](https://realpython.com/beautiful-soup-web-scraper-python/). \n",
    "\n",
    "Please let me know if you find a better resource. I'll share it with the other students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are a data scientist working at HKUST(GZ). Your job is to extract insights from HKUST(GZ) data to answer questions. \n",
    "\n",
    "In this assignment, you will do two tasks. Please recall the high-level data science lifecycle from Lecture 1. I suggest that when doing this assignment, please remind yourself of what data you collected and what questions you tried to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: HKUST(GZ) Information Hub Faculty Members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you don't know what questions to ask. No worries. Start collecting data first. \n",
    "\n",
    "In Task 1, your job is to write a web scraper to extract the faculty information from this page: [https://facultyprofiles.hkust-gz.edu.cn/](https://facultyprofiles.hkust-gz.edu.cn/).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Crawl Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A web page is essentially a file stored in a remote machine (called web server). Please write code to download the HTML page and save it as a text file (\"infhfaculty.html\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T16:15:23.442923Z",
     "start_time": "2023-09-11T16:15:23.439607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML page saved as allinfhfaculty.html\n"
     ]
    }
   ],
   "source": [
    "# write your code\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "URL = \"https://facultyprofiles.hkust-gz.edu.cn\"\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)  # 实例化Chrome浏览器\n",
    "driver.get(URL) # 进入faculty列表\n",
    "\n",
    "driver.implicitly_wait(3) # 设置等待时长\n",
    "info_button = '//*[@id=\"app\"]/section/section/div/ul[1]/li[3]' # 筛选Information Hub faculty\n",
    "driver.find_element(By.XPATH,info_button).click()\n",
    "\n",
    "last_faculty = '//*[@id=\"app\"]/section/section/div/div[2]/div[3]/table/tbody/tr[77]/td[6]/div/button' # inspect可知一共有77位faculty，将加载出最后一位faculty设置为wait条件\n",
    "driver.find_element(By.XPATH,last_faculty)\n",
    "\n",
    "infhfaculty = driver.page_source\n",
    "\n",
    "file_name = \"infhfaculty/allinfhfaculty.html\"  #导出页面\n",
    "with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(infhfaculty)\n",
    "\n",
    "print(\"HTML page saved as allinfhfaculty.html\")\n",
    "\n",
    "soup = BeautifulSoup(infhfaculty, 'html.parser')\n",
    "faculty_list = soup.find_all('div', class_='cell') #查找faculty信息\n",
    "faculty_list = [faculty.text for faculty in faculty_list] #将faculty_list转换为list格式的faculty\n",
    "\n",
    "num_rows = len(faculty_list) // 6 #观察faculty_list，发现每位教授有6个维度的信息\n",
    "num_faculty = num_rows - 1 #len//6-1则是教授的数量\n",
    "faculty_reshaped = pd.DataFrame(pd.Series(faculty_list).values.reshape(num_rows, -1)) #将faculty_list里的数据按顺序排列\n",
    "faculty_table = pd.DataFrame(faculty_reshaped)\n",
    "faculty_table = faculty_table.drop([1,3,5], axis=1) #删除掉不需要的信息维度\n",
    "faculty_table.columns = faculty_table.iloc[0, :] #更改列名\n",
    "faculty_table = faculty_table.drop(0, axis=0) #删除掉第一行的维度名称\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 HTML page saved as Lei CHEN.html\n",
      "2 HTML page saved as Pan HUI.html\n",
      "3 HTML page saved as Vincent Kin Nang LAU.html\n",
      "4 HTML page saved as Irene Man Chi Lo.html\n",
      "5 HTML page saved as Lionel Ming-Shuan NI.html\n",
      "6 HTML page saved as Huamin QU.html\n",
      "7 HTML page saved as Fu-Gee TSUNG.html\n",
      "8 HTML page saved as Hui XIONG.html\n",
      "9 HTML page saved as Liuqing YANG.html\n",
      "10 HTML page saved as Qiang YANG.html\n",
      "11 HTML page saved as Qian ZHANG.html\n",
      "12 HTML page saved as Xiaowen CHU.html\n",
      "13 HTML page saved as Qiong LUO.html\n",
      "14 HTML page saved as Danny Hin Kwok TSANG.html\n",
      "15 HTML page saved as Wei WANG.html\n",
      "16 HTML page saved as Kaishun WU.html\n",
      "17 HTML page saved as Yang YANG.html\n",
      "18 HTML page saved as Kang ZHANG.html\n",
      "19 HTML page saved as Ying CUI.html\n",
      "20 HTML page saved as Xinyi HUANG.html\n",
      "21 HTML page saved as Sung Hun KIM.html\n",
      "22 HTML page saved as DIRK KUTSCHER.html\n",
      "23 HTML page saved as Nan TANG.html\n",
      "24 HTML page saved as Felix Xin WANG.html\n",
      "25 HTML page saved as Sean Sihong XIE.html\n",
      "26 HTML page saved as Tengfei CHANG.html\n",
      "27 HTML page saved as Huangxun CHEN.html\n",
      "28 HTML page saved as Yingcong CHEN.html\n",
      "29 HTML page saved as Mingming FAN.html\n",
      "30 HTML page saved as Zijun GONG.html\n",
      "31 HTML page saved as Varvara GULJAJEVA.html\n",
      "32 HTML page saved as Guobiao HU.html\n",
      "33 HTML page saved as Zhilu LAI.html\n",
      "34 HTML page saved as Jia LI.html\n",
      "35 HTML page saved as Lei LI.html\n",
      "36 HTML page saved as Junwei LIANG.html\n",
      "37 HTML page saved as Yuxuan LIANG.html\n",
      "38 HTML page saved as Hao LIU.html\n",
      "39 HTML page saved as Li LIU.html\n",
      "40 HTML page saved as Yuyu LUO.html\n",
      "41 HTML page saved as RAUL GIANCARLO MARIA MASU.html\n",
      "42 HTML page saved as Ying SUN.html\n",
      "43 HTML page saved as Jing TANG.html\n",
      "44 HTML page saved as Gareth John TYSON.html\n",
      "45 HTML page saved as Hao WANG.html\n",
      "46 HTML page saved as Addison Lin WANG.html\n",
      "47 HTML page saved as Wenjia WANG.html\n",
      "48 HTML page saved as Zeyu WANG.html\n",
      "49 HTML page saved as Zeyi WEN.html\n",
      "50 HTML page saved as Hong XING.html\n",
      "51 HTML page saved as David Kei Man YIP.html\n",
      "52 HTML page saved as Luwen YU.html\n",
      "53 HTML page saved as Wei ZENG.html\n",
      "54 HTML page saved as Theodoros PAPATHEODOROU.html\n",
      "55 HTML page saved as Rui HU.html\n",
      "56 HTML page saved as Jake Junjie ZHANG.html\n",
      "57 HTML page saved as Meihui ZHANG.html\n",
      "58 HTML page saved as Xiao Fang ZHOU.html\n",
      "59 HTML page saved as Xiang CHENG.html\n",
      "60 HTML page saved as Jian GUO.html\n",
      "61 HTML page saved as Feifei LI.html\n",
      "62 HTML page saved as Hang LI.html\n",
      "63 HTML page saved as Shipeng LI.html\n",
      "64 HTML page saved as Jiangchuan LIU.html\n",
      "65 HTML page saved as Tie-Yan LIU.html\n",
      "66 HTML page saved as Dou SHEN.html\n",
      "67 HTML page saved as Keyang TANG.html\n",
      "68 HTML page saved as Da Cheng TAO.html\n",
      "69 HTML page saved as Xiaoyu WANG.html\n",
      "70 HTML page saved as Xing XIE.html\n",
      "71 HTML page saved as Jie Ping YE.html\n",
      "72 HTML page saved as Jing YUAN.html\n",
      "73 HTML page saved as Liang TAN.html\n",
      "74 HTML page saved as Dian ZHANG.html\n",
      "75 HTML page saved as Haijie HU.html\n",
      "76 HTML page saved as Bo Wen ZHOU.html\n",
      "77 HTML page saved as Chen ZHANG.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1, num_faculty+1):\n",
    "    more = '//*[@id=\"app\"]/section/section/div/div[2]/div[3]/table/tbody/tr['+str(i)+']/td[6]/div/button' #点击每一位faculty的more button\n",
    "    driver.find_element(By.XPATH,more).click()\n",
    "    \n",
    "    driver.switch_to.window(driver.window_handles[1]) #切换driver的handle\n",
    "    faculty_table.loc[i,'Profile'] = driver.current_url\n",
    "\n",
    "    for j in range(1,4): #注意到想要获取area的话，需要点击一下 research interest标签，然后再下载整个页面\n",
    "        rs = '//*[@id=\"tab-'+str(j)+'\"]' #但是每位faculty主页的research interest元素的id均不一样，使用class也只获取到空值\n",
    "        try:\n",
    "            # 所以这里采取尝试点击所有与research interest拥有类似xpath的元素，因为它总在标签的最末尾，所以最后能点击到的一定是research interest\n",
    "            driver.find_element(By.XPATH,rs).click()\n",
    "        except NoSuchElementException:\n",
    "            # 在超时后捕获异常并执行下一步\n",
    "            i\n",
    "    \n",
    "    infhfaculty =driver.page_source\n",
    "    \n",
    "    file_name = \"infhfaculty/\"+str(faculty_table.iloc[i-1,0])+\".html\" #导出每一位faculty的profile\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(infhfaculty)\n",
    "    \n",
    "    print(str(i)+\" HTML page saved as \"+str(faculty_table.loc[i,'Name'])+\".html\") \n",
    "    \n",
    "    pw1 = '//*[@id=\"app\"]/section/div/div[2]/div/div/div[3]/div/div/div[3]/a'\n",
    "    pw2 = '//*[@id=\"app\"]/section/div/div[2]/div/div/div[3]/div/div[1]/div[3]/a'\n",
    "    try: #同样，发现faculty的personal web有两种可能的xpath，均加以尝试\n",
    "        driver.find_element(By.XPATH,pw1).click()\n",
    "    except NoSuchElementException:\n",
    "        driver.find_element(By.XPATH,pw2).click()\n",
    "    \n",
    "    driver.switch_to.window(driver.window_handles[2]) #切换driver的handle\n",
    "    faculty_table.loc[i,'Personal Website'] = driver.current_url\n",
    "    \n",
    "    driver.close()\n",
    "    driver.switch_to.window(driver.window_handles[1]) #将handle切换回faculty profiles\n",
    "    \n",
    "    driver.close()\n",
    "    driver.switch_to.window(driver.window_handles[0]) #将handle切换回faculty list\n",
    "\n",
    "driver.quit() #退出chrome浏览器\n",
    "\n",
    "faculty_table.to_csv('faculty_table.csv', index=False) #导出csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Extract Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please write code to extract relevant content (name, rank, area, profile, homepage, ...) from \"infhfaculty.html\" and save them as a CSV file (save as \"faculty_table.csv\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-11T16:19:55.088770Z",
     "start_time": "2023-09-11T16:19:55.083561Z"
    }
   },
   "outputs": [],
   "source": [
    "# write your code\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "faculty_table = pd.read_csv('faculty_table.csv') #导入csv文件\n",
    "\n",
    "name_list = faculty_table.loc[:,'Name']\n",
    "\n",
    "for i in range(0, len(name_list)): #循环打开下载的faculty profiles\n",
    "    file_name = 'infhfaculty/'+str(name_list.loc[i])+'.html'\n",
    "\n",
    "    with open(file_name, 'r', encoding='utf-8') as file: # 打开html文件\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    faculty_info = soup.find('p', class_='icon-text') #查找faculty的office信息\n",
    "    office =  [office_info for office_info in faculty_info] #转换为list格式的faculty\n",
    "    if len(office) > 0:\n",
    "        faculty_table.loc[i, 'Office'] = office\n",
    "    else:\n",
    "        faculty_table.loc[i, 'Office'] = 'NaN'\n",
    "    \n",
    "    faculty_info = soup.find_all('p', class_='content') #查找faculty的area信息\n",
    "    area = [area_info for area_info in faculty_info]\n",
    "\n",
    "    areas = pd.DataFrame(area)\n",
    "    if areas.shape[1] > 0: #faculty往往拥有多个研究兴趣，将其组合成string\n",
    "        area_str = str()\n",
    "        for j in range(0,len(areas)):\n",
    "            area_str = str(area_str) + str(areas.loc[j, 0]) + \", \"\n",
    "        faculty_table.loc[i,'Area'] = area_str\n",
    "    else:\n",
    "        faculty_table.loc[i,'Area'] = 'NaN'\n",
    "\n",
    "faculty_table.to_csv('faculty_table.csv', index=False) #输出到csv中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Interesting Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you don't need to do anything for Task 1(c). The purpose of this part is to give you some sense about how to leverage Exploratory Data Analysis (EDA) to come up with interesting questions about the data. EDA is an important topic in data science; you will  learn it soon from this course. \n",
    "\n",
    "\n",
    "First, please install [dataprep](http://dataprep.ai).\n",
    "Then, run the cell below. \n",
    "It shows a bar chart for every column. What interesting findings can you get from these visualizations? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some examples:\n",
    "\n",
    "**Finding 1:** Assistant Professor# (~76) is more than 5x larger than Associate Professor# (10). \n",
    "\n",
    "**Questions:** Why did it happen? Is it common in all CS schools in the world? Will the gap go larger or smaller in five years? What actions can be taken to enlarge/shrink the gap?\n",
    "\n",
    "\n",
    "**Finding 2:** The Homepage has 22% missing values. \n",
    "\n",
    "**Questions:** Why are there so many missing values? Is it because many faculty do not have their own homepages or do not add their homepages to the school page? What actions can be taken to avoid this to happen in the future? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Age Follows Normal Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you start with a question and then figure out what data to collect.\n",
    "\n",
    "The question that you are interested in is `Does HKUST(GZ) Info Hub faculty age follow a normal distribution?`\n",
    "\n",
    "To estimate the age of a faculty member, you can collect the year in which s/he graduates from a university (`gradyear`) and then estimate `age` using the following equation:\n",
    "\n",
    "$$age \\approx 2023+23 - gradyear$$\n",
    "\n",
    "For example, if one graduates from a university in 1990, then the age is estimated as 2023+23-1990 = 56. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Crawl Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You notice that faculty profile pages contain graduation information. For example, you can see that Dr. Yuyu LUO graduated from Tsinghua University in 2023 at [https://facultyprofiles.hkust-gz.edu.cn/faculty-personal-page/LUO-Yuyu/yuyuluo](https://facultyprofiles.hkust-gz.edu.cn/faculty-personal-page/LUO-Yuyu/yuyuluo). \n",
    "\n",
    "\n",
    "Please write code to download the profile pages (info hub faculties) and save each page as a text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML page saved as allinfhfaculty.html\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "URL = \"https://facultyprofiles.hkust-gz.edu.cn\"\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)  # 实例化Chrome浏览器\n",
    "driver.get(URL) # 进入faculty列表\n",
    "\n",
    "wait = WebDriverWait(driver, 10) # 设置等待时长\n",
    "info_button = '//*[@id=\"app\"]/section/section/div/ul[1]/li[3]' # 筛选Information Hub faculty\n",
    "driver.find_element(By.XPATH,info_button).click()\n",
    "\n",
    "last_faculty = '//*[@id=\"app\"]/section/section/div/div[2]/div[3]/table/tbody/tr[77]/td[6]/div/button' # inspect可知一共有77位faculty，将加载出最后一位faculty设置为wait条件\n",
    "wait.until(EC.element_to_be_clickable((By.XPATH,last_faculty)))\n",
    "\n",
    "infhfaculty = driver.page_source\n",
    "\n",
    "file_name = \"infhfaculty/allinfhfaculty.html\"  #导出页面\n",
    "with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(infhfaculty)\n",
    "\n",
    "print(\"HTML page saved as allinfhfaculty.html\")\n",
    "\n",
    "soup = BeautifulSoup(infhfaculty, 'html.parser')\n",
    "faculty_list = soup.find_all('div', class_='cell') #查找faculty信息\n",
    "faculty_list = [faculty.text for faculty in faculty_list] #将faculty_list转换为list格式的faculty\n",
    "\n",
    "num_rows = len(faculty_list) // 6 #观察faculty_list，发现每位教授有6个维度的信息\n",
    "num_faculty = num_rows - 1 #len//6-1则是教授的数量\n",
    "faculty_reshaped = pd.DataFrame(pd.Series(faculty_list).values.reshape(num_rows, -1)) #将faculty_list里的数据按顺序排列\n",
    "faculty_table = pd.DataFrame(faculty_reshaped)\n",
    "faculty_table = faculty_table.drop([1,2,3,4,5], axis=1) #删除掉不需要的信息维度\n",
    "faculty_table.columns = faculty_table.iloc[0, :]\n",
    "faculty_table = faculty_table.drop(0, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 HTML page saved as Lei CHEN.html\n",
      "2 HTML page saved as Pan HUI.html\n",
      "3 HTML page saved as Vincent Kin Nang LAU.html\n",
      "4 HTML page saved as Irene Man Chi Lo.html\n",
      "5 HTML page saved as Lionel Ming-Shuan NI.html\n",
      "6 HTML page saved as Huamin QU.html\n",
      "7 HTML page saved as Fu-Gee TSUNG.html\n",
      "8 HTML page saved as Hui XIONG.html\n",
      "9 HTML page saved as Liuqing YANG.html\n",
      "10 HTML page saved as Qiang YANG.html\n",
      "11 HTML page saved as Qian ZHANG.html\n",
      "12 HTML page saved as Xiaowen CHU.html\n",
      "13 HTML page saved as Qiong LUO.html\n",
      "14 HTML page saved as Danny Hin Kwok TSANG.html\n",
      "15 HTML page saved as Wei WANG.html\n",
      "16 HTML page saved as Kaishun WU.html\n",
      "17 HTML page saved as Yang YANG.html\n",
      "18 HTML page saved as Kang ZHANG.html\n",
      "19 HTML page saved as Ying CUI.html\n",
      "20 HTML page saved as Xinyi HUANG.html\n",
      "21 HTML page saved as Sung Hun KIM.html\n",
      "22 HTML page saved as DIRK KUTSCHER.html\n",
      "23 HTML page saved as Nan TANG.html\n",
      "24 HTML page saved as Felix Xin WANG.html\n",
      "25 HTML page saved as Sean Sihong XIE.html\n",
      "26 HTML page saved as Tengfei CHANG.html\n",
      "27 HTML page saved as Huangxun CHEN.html\n",
      "28 HTML page saved as Yingcong CHEN.html\n",
      "29 HTML page saved as Mingming FAN.html\n",
      "30 HTML page saved as Zijun GONG.html\n",
      "31 HTML page saved as Varvara GULJAJEVA.html\n",
      "32 HTML page saved as Guobiao HU.html\n",
      "33 HTML page saved as Zhilu LAI.html\n",
      "34 HTML page saved as Jia LI.html\n",
      "35 HTML page saved as Lei LI.html\n",
      "36 HTML page saved as Junwei LIANG.html\n",
      "37 HTML page saved as Yuxuan LIANG.html\n",
      "38 HTML page saved as Hao LIU.html\n",
      "39 HTML page saved as Li LIU.html\n",
      "40 HTML page saved as Yuyu LUO.html\n",
      "41 HTML page saved as RAUL GIANCARLO MARIA MASU.html\n",
      "42 HTML page saved as Ying SUN.html\n",
      "43 HTML page saved as Jing TANG.html\n",
      "44 HTML page saved as Gareth John TYSON.html\n",
      "45 HTML page saved as Hao WANG.html\n",
      "46 HTML page saved as Addison Lin WANG.html\n",
      "47 HTML page saved as Wenjia WANG.html\n",
      "48 HTML page saved as Zeyu WANG.html\n",
      "49 HTML page saved as Zeyi WEN.html\n",
      "50 HTML page saved as Hong XING.html\n",
      "51 HTML page saved as David Kei Man YIP.html\n",
      "52 HTML page saved as Luwen YU.html\n",
      "53 HTML page saved as Wei ZENG.html\n",
      "54 HTML page saved as Theodoros PAPATHEODOROU.html\n",
      "55 HTML page saved as Rui HU.html\n",
      "56 HTML page saved as Jake Junjie ZHANG.html\n",
      "57 HTML page saved as Meihui ZHANG.html\n",
      "58 HTML page saved as Xiao Fang ZHOU.html\n",
      "59 HTML page saved as Xiang CHENG.html\n",
      "60 HTML page saved as Jian GUO.html\n",
      "61 HTML page saved as Feifei LI.html\n",
      "62 HTML page saved as Hang LI.html\n",
      "63 HTML page saved as Shipeng LI.html\n",
      "64 HTML page saved as Jiangchuan LIU.html\n",
      "65 HTML page saved as Tie-Yan LIU.html\n",
      "66 HTML page saved as Dou SHEN.html\n",
      "67 HTML page saved as Keyang TANG.html\n",
      "68 HTML page saved as Da Cheng TAO.html\n",
      "69 HTML page saved as Xiaoyu WANG.html\n",
      "70 HTML page saved as Xing XIE.html\n",
      "71 HTML page saved as Jie Ping YE.html\n",
      "72 HTML page saved as Jing YUAN.html\n",
      "73 HTML page saved as Liang TAN.html\n",
      "74 HTML page saved as Dian ZHANG.html\n",
      "75 HTML page saved as Haijie HU.html\n",
      "76 HTML page saved as Bo Wen ZHOU.html\n",
      "77 HTML page saved as Chen ZHANG.html\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "for i in range(1, num_faculty+1):\n",
    "    more = '//*[@id=\"app\"]/section/section/div/div[2]/div[3]/table/tbody/tr['+str(i)+']/td[6]/div/button' #点击每一位faculty的more button\n",
    "    driver.find_element(By.XPATH,more).click()\n",
    "    \n",
    "    driver.switch_to.window(driver.window_handles[1]) #切换driver的handle\n",
    "    element = '//*[@id=\"tab-0\"]'\n",
    "    wait.until(EC.element_to_be_clickable((By.XPATH,element))).click() #设置等待条件，否则会下载空html\n",
    "    infhfaculty =driver.page_source\n",
    "    \n",
    "    file_name = \"infhfaculty/\"+str(faculty_table.iloc[i-1,0])+\".html\" #导出每一位faculty的profile\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(infhfaculty)\n",
    "    \n",
    "    print(str(i)+\" HTML page saved as \"+str(faculty_table.loc[i,'Name'])+\".html\") \n",
    "    \n",
    "    driver.close()\n",
    "    driver.switch_to.window(driver.window_handles[0]) #将handle切换回faculty list\n",
    "\n",
    "driver.quit() #退出chrome浏览器\n",
    "\n",
    "faculty_table.to_csv('faculty_table.csv', index=False) #导出csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Extract Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please write code to extract the earliest graduation year (e.g., 2023 for Dr. Yuyu LUO) from each profile page, and create a csv file like [faculty_grad_year.csv](./faculty_grad_year.csv). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "faculty_table = pd.read_csv('faculty_table.csv') #导入csv文件\n",
    "\n",
    "faculty_list = faculty_table.loc[:,'Name']\n",
    "grad_year = pd.DataFrame(columns=['Name', 'Graduate Year'])\n",
    "\n",
    "for i in range(0,len(faculty_list)):\n",
    "    file_name = 'infhfaculty/'+str(faculty_list.loc[i])+'.html'\n",
    "\n",
    "    with open(file_name, 'r', encoding='utf-8') as file: # 打开html文件\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    faculty_detail = soup.find('div', class_='degree-detail') #查找faculty的office信息\n",
    "    details =  [detail for detail in faculty_detail] #转换为list格式的faculty\n",
    "\n",
    "    if len(details) > 0:\n",
    "        raw = str(details[1])\n",
    "        data = raw.split(',')\n",
    "        data = pd.DataFrame(str(data[len(data)-1]).split('<'))\n",
    "        grad_year.loc[i,:] = [faculty_list.loc[i], data.loc[0]]\n",
    "        \n",
    "grad_year.to_csv('faculty_grad_year.csv', index=False) #导出csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Interesting Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Task 1(c), you don't need to do anything here. Just look at different visualizations w.r.t. age and give yourself an answer to the question: `Does HKUST(GZ) Info Hub faculty age follow a normal distribution?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataprep.eda import plot\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"faculty_grad_year.csv\")\n",
    "df[\"age\"] = 2023+23-df[\"gradyear\"]\n",
    "\n",
    "plot(df, \"age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code in this notebook, and submit it to the Canvas assignment `Assignment 1`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
